% This is part of (everything) I know in mathematics
% Copyright (c) 2011-2019
%   Laurent Claessens
% See the file fdl-1.3.txt for copying conditions.


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Calcul de limites}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Beaucoup de techniques de calcul de limites fonctionnent bien avec les fonctions trigonométriques, entre autres grâce à l'utilisation des coordonnées polaires de la proposition~\ref{PROPooFLUAooDsyMXO}. De plus, le théorème de la fonction implicite Nous en voyons quelques exemples à présent.

\begin{example}[Limite et prolongement par continuité] \label{ExQWHooGddTLE}
    La fonction
    \begin{equation}
        f(x)=\frac{ \cos(x)-1 }{ x }
    \end{equation}
    n'est pas définie en \( x=0\).

    Nous avons vu dans l'équation \eqref{SUBEQooTTNNooXzApSM} que \( \cos(0)=1\), donc la limite
    \begin{equation}
        \lim_{x\to 0} \frac{ \cos(x)-1 }{ x }
    \end{equation}
    est la limite définissant la dérivée de cosinus en \( 0\) (ici, le \( x\) joue le rôle de \( \epsilon\)). Le lemme~\ref{LEMooBBCAooHLWmno} nous donne la dérivée du cosinus comme étant le sinus. Nous avons donc :
    \begin{equation}
        \lim_{x\to 0} \frac{ \cos(x)-1 }{ x }=\sin(0)=0,
    \end{equation}
    et nous définissons le prolongement par continuité :
    \begin{equation}
        \tilde f(x)=\begin{cases}
            \frac{ \cos(x)-1 }{ x }    &   \text{si } x\neq 0\\
            0    &    \text{sinon}.
        \end{cases}
    \end{equation}

    Encore une fois, le graphe de la fonction \(\tilde f\) ne présente aucune particularité autour de \( x=0\).
    \begin{center}
        \input{auto/pictures_tex/Fig_RPNooQXxpZZ.pstricks}
    \end{center}
\end{example}

\begin{example}[Un calcul heuristique de limite]        \label{EXooINLRooPzRWEA}
    Soit à calculer la limite suivante :
    \begin{equation}
        \lim_{x\to 0} \frac{  e^{-2\cos(x)+2}\sin(x) }{ \sqrt{ e^{2\cos(x)+2}}-1 }.
    \end{equation}
    La stratégie que nous allons suivre pour calculer cette limite est de développer certaines parties de l'expression en série de Taylor, afin de simplifier l'expression. La première chose à faire est de remplacer $ e^{y(x)}$ par $1+y(x)$ lorsque $y(x)\to 0$. La limite devient
    \begin{equation}
        \lim_{x\to 0} \frac{ \big( -2\cos(x)+3 \big)\sin(x) }{ \sqrt{-2\cos(x)+2} }.
    \end{equation}
    Nous allons maintenant remplacer $\cos(x)$ par $1$ au numérateur et par $1-x^2/2$ au dénominateur. Pourquoi ? Parce que le cosinus du dénominateur est dans une racine, donc nous nous attendons à ce que le terme de degré deux du cosinus donne un degré un en dehors de la racine, alors que du degré un est exactement ce que nous avons au numérateur : le développement du sinus commence par $x$.

    Nous calculons donc
    \begin{equation}
        \begin{aligned}[]
            \lim_{x\to 0} \frac{ \sin(x) }{ \sqrt{-2\left( 1-\frac{ x^2 }{ 2 } \right)+2} }=\lim_{x\to 0} \frac{ \sin(x) }{ x }=1.
        \end{aligned}
    \end{equation}
    Tout ceci n'est évidemment pas très rigoureux, mais en principe vous avez tous les éléments en main pour justifier les étapes.
\end{example}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode des coordonnées polaires}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooWCGMooPrXSpt}

La proposition suivante exprime la définition de la limite en d'autres termes, et va être pratique dans le calcul de certaines limites.
\begin{proposition}		\label{PropMethodePolaire}
	Soit $f\colon D\subset\eR^m\to \eR^n$, $a$ un point d'accumulation de $D$ et $\ell\in \eR^n$. Nous définissons
	\begin{equation}
		E_r=\{ f(x)\tq x\in B(a,r)\cap D \},
	\end{equation}
	et
	\begin{equation}
		s_r=\sup\{ \| v-\ell \|\tq v\in E_r \}.
	\end{equation}
	Alors nous avons $\lim_{x\to a} f(x)=\ell$ si et seulement si $\lim_{r\to 0} s_r=0$.
\end{proposition}

Dans cette proposition, $E_r$ représente l'ensemble des valeurs atteintes par $f$ dans un rayon $r$ autour de $a$. Le nombre $s_r$ sélectionne, parmi toutes ces valeurs, celle qui est la plus éloignée de $\ell$ et donne la distance. En d'autres termes, $s_r$ est la distance maximale entre $f(x)$ et $\ell$ lorsque $x$ est à une distance au maximum $r$ de $a$.

Lorsque nous avons affaire à une fonction $f\colon \eR^2\to \eR$, cette proposition nous permet de calculer facilement les limites en passant aux coordonnées polaires.

\begin{example}		\label{ExempleMethodeTrigigi}
	Reprenons la fonction de l'exemple~\ref{ExFNExempleMethodeTrigigi}:
	\begin{equation}
		f(x,y)=\frac{ xy }{ x^2+y^2 }.
	\end{equation}
	Son domaine est $\eR^2\setminus\{ (0,0) \}$. Nous voulons calculer $\lim_{(x,y)\to(0,0)}f(x,y)$. Écrivons la définition de $E_r$~:
	\begin{equation}
		E_r=\{ f(x,y)\tq (x,y)\in B\big( (0,0),r \big) \}.
	\end{equation}
	Les points de la boule sont, en coordonnées polaires, les points de la forme $(\rho,\theta)$ avec $\rho<r$. La chose intéressante est que $f(\rho,\theta)$ est relativement simple (plus simple que la fonction départ). En effet en remplaçant tous les $x$ par $\rho\cos(\theta)$ et tous les $y$ par $\rho\sin(\theta)$, et en utilisant le fait que $\cos^2(\theta)+\sin^2(\theta)=1$, nous trouvons
	\begin{equation}		\label{Eq2807fpolairerhodeuxcossin}
		f(\rho,\theta)=\frac{ \rho^2\cos(\theta)\sin(\theta) }{ \rho^2 }=\cos(\theta)\sin(\theta).
	\end{equation}
	Cela signifie que
	\begin{equation}
		E_r=\{ \cos(\theta)\sin(\theta)\tq\theta\in\mathopen[ 0 , 2\pi [ \}.
	\end{equation}
	Prenons $\ell$ quelconque. Le nombre $s_r$ est le supremum des
	\begin{equation}
		\| \ell-\cos(\theta)\sin(\theta) \|
	\end{equation}
	lorsque $\theta$ parcours $\mathopen[ 0 , 2\pi \mathclose]$. Nous ne sommes pas obligés calculer la valeur exacte de $s_r$. Ce qui compte ici est que $s_r$ ne vaut certainement pas zéro, et ne dépend pas de $r$. Donc il est impossible d'avoir $\lim_{r\to 0} s_r=0$, et la fonction donnée n'a pas de limite en $(0,0)$.
\end{example}

Nous pouvons retenir cette règle pour calculer les limites lorsque $(x,y)\to(0,0)$ de fonctions $f\colon \eR^2\to \eR$ :
\begin{enumerate}
	\item
		passer en coordonnées polaires, c'est-à-dire remplacer $x$ par $\rho\cos(\theta)$ et $y$ par $\rho\sin(\theta)$;
	\item
		nous obtenons une fonction $g$ de $\rho$ et $\theta$. Si la limite $\lim_{r\to 0} g(r,\theta)$ n'existe pas ou dépend de $\theta$, alors la fonction n'a pas de limite. Si on peut majorer $g$ par une fonction ne dépendant pas de $\theta$, et que cette fonction a une limite lorsque $r\to 0$, alors cette limite est la limite de la fonction.
\end{enumerate}

La vraie difficulté de la technique des coordonnées polaires est de trouver le supremum de $E_r$, ou tout au moins de montrer qu'il est borné par une fonction qui a une limite qui ne dépend pas de $\theta$. Une des situations classiques dans laquelle c'est facile est lorsque la fonction se présente comme une fonction de $r$ multiplié par une fonction de $\theta$.

\begin{example}		\label{Exemplexyxsqysq}
	Soit à calculer la limite
	\begin{equation}
		\lim_{(x,y)\to(0,0)}xy\left( \frac{ x^2-y^2 }{ x^2+y^2 }\right).
	\end{equation}
	Le passage aux coordonnées polaires donne
	\begin{equation}
		f(r,\theta)=r^2\sin\theta\cos\theta(\cos^2\theta-\sin^2\theta).
	\end{equation}
	Déterminer le supremum de cela est relativement difficile. Mais nous savons que de toutes façons, la quantité $\sin\theta\cos\theta(\cos^2\theta-\sin^2\theta)$ est bornée par $1$. Donc
	\begin{equation}
		\| f(r,\theta) \|\leq r^2.
	\end{equation}
	Maintenant la règle de l'étau montre que $\lim_{(x,y)\to(0,0)}f(x,y)$ est zéro.

	La situation vraiment gênante serait celle avec une fonction de $\theta$ qui risque de s'annuler dans un dénominateur.
\end{example}

L'exemple~\ref{EXooSDHDooJzDioW} donnera un cas où la méthode fonctionne plus difficilement. Entre autres parce qu'il utilisera en même temps la méthode des chemins et celle des coordonnées polaires.

\begin{example}\label{ExmeASDLAf}
	Considérons fonction
	\begin{equation}
		f(x,y)=\frac{ x^2+y^2 }{ x-y }.
	\end{equation}
	Une mauvaise idée pour prouver que la limite n'existe pas pour $(x,y)\to(0,0)$ est de considérer le chemin $(t,t)$. En effet, la fonction n'existe pas sur ce chemin. Or la méthode des chemins parle uniquement de chemins contenus dans le domaine de la fonction.

	Nous prouvons que la limite n'existe pas en trouvant des chemins le long desquels les limites sont différentes. Si nous essayons le chemin \( (t,kt)\) avec \( k\) constant, nous trouvons
    \begin{equation}
        f(t,kt)=\frac{ t(1+k^2) }{ 1-k }.
    \end{equation}
    La limite \( t\to 0\) est hélas toujours \( 0\). Nous ne pouvons donc pas conclure.

    Nous allons maintenant utiliser la même technique que celle utilisée en coordonnées polaires. Vous noterez que dans ce cas, travailler en cartésiennes donne lieu à des calculs plus longs.  L'astuce consiste à prendre \( k\) non constant et à chercher par exemple \( k(t)\) de façon à avoir
    \begin{equation}
        \frac{ 1+k(t)^2 }{ 1-k(t) }=\frac{1}{ t }.
    \end{equation}
    Avec une telle fonction \( k\), la fonction \( t\mapsto f(t,tk(t))\) serait la constante \( 1\). L'équation à résoudre pour \( k\) est
    \begin{equation}
        tk^2+k+(t-1)=0,
    \end{equation}
    et les solutions sont
    \begin{equation}
        k(t)=\frac{ -1\pm\sqrt{1-4t(t-1)} }{ 2t }.
    \end{equation}
    Nous proposons donc les chemins
    \begin{equation}
        \begin{pmatrix}
            x    \\
            y
        \end{pmatrix}=\begin{pmatrix}
            t    \\
            \frac{ -1\pm\sqrt{1-4t(t-1)}    }{2}
        \end{pmatrix}
    \end{equation}
    Nous devons vérifier deux points. D'abord que ce chemin est bien défini, et ensuite que \( tk(t)\) tend bien vers zéro lorsque \( t\to 0\) (sinon \( (t,k(t)t)\)) n'est pas un chemin passant par \( (0,0)\). Lorsque \( t\) est petit, ce qui se trouve sous la racine est proche de \( 1\) et ne pose pas de problèmes. Ensuite,
    \begin{equation}
        \lim_{t\to 0} tk(t)=\frac{ -1\pm 1 }{ 2 }.
    \end{equation}
    En choisissant le signe \( +\), nous trouvons un chemin qui nous convient.

    Ce que nous avons prouvé est que
    \begin{equation}
        f\left( t,   \frac{ -1+\sqrt{1-4t(t-1)}    }{2}\right)=1
    \end{equation}
    pour tout \( t\). Le long de ce chemin, la limite de \( f\) est donc \( 1\). Cette limite est différente des limites obtenues le long de chemins avec \( k\) constant. La limite \( \lim_{(x,y)\to (0,0)} f(x,y)\) n'existe donc pas.
\end{example}

\begin{example}\label{seno}
	Considérons la fonction (figure~\ref{LabelFigsenotopologo})

	\begin{equation}
		f(x,y)=\begin{cases}
			\sqrt{x^2+y^2}\sin\frac{1}{ x^2+y^2 }	&	\text{si }(x,y)\neq(0,0)\\
			0	&	 \text{si }(x,y)=(0,0),
		\end{cases}
	\end{equation}
    et cherchons la limite $(x,y)\to(0,0)$. Le passage en coordonnées polaires\footnote{Proposition~\ref{PROPooFLUAooDsyMXO}.} donne
	\begin{equation}		\label{EqFoncRho2907}
		f(\rho,\theta)=\rho\sin\frac{1}{ \rho }.
	\end{equation}
	Pour calculer la limite de cela lorsque $\rho\to 0$, nous remarquons que
	\begin{equation}
		0\leq|\rho\sin\frac{1}{ \rho }|\leq\rho
	\end{equation}
	parce que $\sin(\frac{1}{ \rho })\leq 1$ quel que soit $\rho$. Or évidemment $\lim_{\rho\to 0} \rho=0$, donc la limite de la fonction \eqref{EqFoncRho2907} est zéro et ne dépend pas de $\theta$. Nous en concluons que $\lim_{(x,y)\to(0,0)}f(x,y)=0$.
\end{example}
\newcommand{\CaptionFigsenotopologo}{La fonction de l'exemple~\ref{seno}.}
\input{auto/pictures_tex/Fig_senotopologo.pstricks}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Méthode du développement asymptotique}
%---------------------------------------------------------------------------------------------------------------------------
\label{SUBSECooRAKKooAnpvkE}

Nous savons  que nous pouvons développer certaines fonctions en série grâce au développement de Taylor (théorème~\ref{ThoTaylor}). Lorsque nous avons une limite à calculer, nous pouvons remplacer certaines parties de la fonction à traiter par la formule \eqref{subeqfTepseqb}. Cela est très utile pour comparer des fonctions trigonométrique à des polynômes.

\begin{lemma}       \label{LEMooZYNEooYkwsWD}
    Nous avons la limite
    \begin{equation}
        \lim_{x\to 0} \frac{ \sin(x) }{ x }=1.
    \end{equation}
\end{lemma}

\begin{proof}
    Une manière de prouver cela est d'écrire
    \begin{equation}
		\sin(x)=x+h(x)
	\end{equation}
	avec $h\in o(x)$, c'est-à-dire $\lim_{x\to 0} h(x)/x=0$. Alors nous avons
	\begin{equation}
		\lim_{x\to 0} \frac{ \sin(x) }{ x }=\lim_{x\to 0} \frac{ x+h(x) }{ x }=\lim_{x\to 0} \frac{ x }{ x }+\lim_{x\to 0} \frac{ h(x) }{ x }=1.
	\end{equation}
\end{proof}

L'utilisation de la proposition~\ref{PropLimCompose} permet d'utiliser cette technique dans le cadre de limites à plusieurs variables. Reprenons le lemme \ref{LEMooZYNEooYkwsWD} un tout petit peu modifié :

\begin{lemma}       \label{LEMooSFALooVRBdNb}
    Pour tout \( x>0\) nous avons \( \sin(x)<x\).
\end{lemma}

\begin{proof}
    Nous posons \( f(x)=x-\sin(x)\). Cette fonction vérifie \( f(0)=0\) et
    \begin{equation}
        f'(x)=1-\cos(x).
    \end{equation}
    Vu que \( | \cos(x) |\leq 1\), nous avons toujours \( f'(x)\geq 0\) et même \( f'(x)>0\) pour \( x\in \mathopen] 0 , \delta \mathclose]\). Donc \( f\) est au moins strictement croissante sur \( \mathopen] 0 , \delta \mathclose]\) et ensuite strictement croissante presque partout.
\end{proof}

\begin{example}
	Soit à calculer $\lim_{(x,y)\to(0,0)}f(x,y)$ où
	\begin{equation}
		f(x,y)=\frac{ \sin(xy) }{ xy }.
	\end{equation}
	La première chose à faire est de voir $f$ comme la composée de fonctions $f=f_1\circ f_2$ avec
	\begin{equation}
		\begin{aligned}
			f_1\colon \eR&\to \eR \\
			t&\mapsto \frac{ \sin(t) }{ t }
		\end{aligned}
	\end{equation}
	et
	\begin{equation}
		\begin{aligned}
			f_2\colon \eR^2&\to \eR \\
			(x,y)&\mapsto xy.
		\end{aligned}
	\end{equation}
	Étant donné que $\lim_{(x,y)\to(0,0)}f_2(x,y)=0$, nous avons $\lim_{(x,y)\to(0,0)}f(x,y)=\lim_{t\to 0} f_1(t)=1$.
\end{example}

\begin{example}     \label{EXooETZYooYsKPDJ}
Les dérivées partielles de la fonction $f(x,y)=xy^3+\sin y$ au point $(0,\pi)$ sont
\[
\partial_xf(0,\pi)=\frac{ \partial f }{ \partial x }(0,\pi)=\lim_{\begin{subarray}{l}
    t\to 0\\ t\neq 0
  \end{subarray}} \frac{(t\pi^3+\sin \pi)-(\sin \pi)}{t}= \pi^3,
\]
\[
\partial_yf(0,\pi)=\frac{ \partial f }{ \partial y }(0,\pi)=\lim_{\begin{subarray}{l}
    t\to 0\\ t\neq 0
  \end{subarray}} \frac{0(\pi+t)^3+\sin (t+\pi)-0\cdot \pi^3}{t}= \cos \pi=-1,
\]
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Quelques intégrales avec de la trigonométrie}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SECooOOPPooZLbaEH}

Le théorème~\ref{THOooUMIWooZUtUSg} manque un peu d'exemples. Nous allons en voir quelques-uns maintenant.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Changement de variables}
%---------------------------------------------------------------------------------------------------------------------------

Le domaine $E=\{ (x,y)\in\eR^2\tq x^2+y^2<1 \}$ s'écrit plus facilement $E=\{ (r,\theta)\tq r<1 \}$ en coordonnées polaires. Le passage aux coordonnées polaires permet de transformer une intégration sur un domaine rond à une intégration sur le domaine rectangulaire $\mathopen]0,2\pi\mathclose[\times\mathopen]0,1\mathclose[$. La question est évidemment de savoir si nous pouvons écrire
\begin{equation}
    \int_Ef=\int_{0}^{2\pi}\int_0^1f(r\cos\theta,r\sin\theta)drd\theta.
\end{equation}
Hélas, non; la vie n'est pas aussi simple.

\begin{definition}
    Un \defe{difféomorphisme}{difféomorphisme} est une application $g\colon A\to B$ telle que $g$ et $g^{-1}\colon B\to A$ soient de classe $C^1$.
\end{definition}

\begin{theorem}
Soit $g\colon A\to B$ un difféomorphisme. Soient $F\subset B$ un ensemble mesurable et borné et $f\colon F\to \eR$ une fonction bornée et intégrable. Supposons que $g^{-1}(F)$ soit borné et que $Jg$ soit borné sur $g^{-1}(F)$. Alors
\begin{equation}
	\int_Ff(x)dy=\int_{g^{-1}(F)f\big( g(x) \big)}| Jg(x) |dx
\end{equation}
\end{theorem}
Pour rappel, $Jg$ est le déterminant de la matrice \href{http://fr.wikipedia.org/wiki/Matrice_jacobienne}{jacobienne} (aucun lien de \href{http://fr.wikipedia.org/wiki/Jacob}{parenté}) donnée par
\begin{equation}
	Jg=\det\begin{pmatrix}
	\partial_xg_1	&	\partial_yg_1	\\
	\partial_xg_2	&	\partial_tg_2
\end{pmatrix}.
\end{equation}

Comme dans les intégrales simples, il y a souvent moyen de trouver un changement de variables qui simplifie les expressions.  Le domaine $E=\{ (x,y)\in\eR^2\tq x^2+y^2<1 \}$ par exemple s'écrit plus facilement $E=\{ (r,\theta)\tq r<1 \}$ en coordonnées polaires. Le passage aux coordonnées polaires permet de transformer une intégration sur un domaine rond à une intégration sur le domaine rectangulaire $\mathopen]0,2\pi\mathclose[\times\mathopen]0,1\mathclose[$. La question est évidemment de savoir si nous pouvons écrire
\begin{equation}
	\int_Ef=\int_{0}^{2\pi}\int_0^1f(r\cos\theta,r\sin\theta)drd\theta.
\end{equation}
Hélas ce n'est pas le cas. Il faut tenir compte du fait que le changement de base dilate ou contracte certaines surfaces.

Soit $\varphi\colon D_1\subset\eR^2\to D_2\subset \eR^2$ une fonction bijective de classe $C^1$ dont l'inverse est également de classe $C^1$. On désigne par $x$ et $y$ ses composantes, c'est-à-dire que
\begin{equation}
    \varphi(u,v)=\begin{pmatrix}
        x(u,v)    \\
        y(u,v)
    \end{pmatrix}
\end{equation}
avec $(u,v)\in D_1$.

\begin{theorem}     \label{ThoChamDeVarIntDDf}
    Soit une fonction continue $f\colon D_2\to \eR$. Alors
    \begin{equation}
        \int_{\varphi(D_1)}f(x,y)dxdy=\int_{D_1}f\big( x(u,v),y(u,v) \big)| J_{\varphi}(u,v) |dudv
    \end{equation}
    où $J_{\varphi}$ est le Jacobien de $\varphi$ c'est-à-dire
    \begin{equation}
        J_{\varphi}(u,v)=\det\begin{pmatrix}
            \frac{ \partial x }{ \partial u }    &   \frac{ \partial x }{ \partial v }    \\
            \frac{ \partial y }{ \partial u }    &   \frac{ \partial u }{ \partial v }
        \end{pmatrix}.
    \end{equation}
\end{theorem}
Ne pas oublier de prendre la valeur absolue lorsqu'on utilise le Jacobien dans un changement de variables.

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Coordonnées polaires}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    Calculons la surface du disque $D$ de rayon $R$. Nous devons calculer
    \begin{equation}
        \int_Ddxdy.
    \end{equation}
    Pour passer au polaires, nous savons que le disque est décrit par
    \begin{equation}
        D=\{ (r,\theta)\tq 0\leq r\leq R,0\leq\theta\leq 2\pi \}.
    \end{equation}
    Nous avons donc
    \begin{equation}
        \int_Ddxdy=\int_{D}r\,drd\theta=\int_0^{2\pi}\int_0^Rr\,drd\theta=2\pi\int_0^Rr\,dr=\pi R^2.
    \end{equation}
\end{example}

\begin{example}     \label{ExpmfDtAtV}
    Montrons comment intégrer la fonction $f(x,y)=\sqrt{1-x^2-y^2}$ sur le domaine délimité par la droite $y=x$ et le cercle $x^2+y^2=y$, représenté sur la figure~\ref{LabelFigHFAYooOrfMAA}. Pour trouver le centre et le rayon du cercle $x^2+y^2=y$, nous commençons par écrire $x^2+y^2-y=0$, et ensuite nous reformons le carré : $y^2-y=(y-\frac{ 1 }{2})^2-\frac{1}{ 4 }$.

\newcommand{\CaptionFigHFAYooOrfMAA}{Passage en polaire pour intégrer sur un morceau de cercle.}
\input{auto/pictures_tex/Fig_HFAYooOrfMAA.pstricks}

    Le passage en polaire transforme les équations du bord du domaine en
    \begin{equation}
        \begin{aligned}[]
            \cos(\theta)&=\sin(\theta)\\
            r^2&=r\sin(\theta).
        \end{aligned}
    \end{equation}
    L'angle $\theta$ parcours donc $\mathopen] 0 , \pi/4 \mathclose[$, et le rayon, pour chacun de ces $\theta$ parcours $\mathopen] 0 , \sin(\theta) \mathclose[$. La fonction à intégrer se note maintenant $f(r,\theta)=\sqrt{1-r^2}$. Donc l'intégrale à calculer est
    \begin{equation}		\label{PgOMRapIntMultFubiniBoutCercle}
        \int_{0}^{\pi/4}\left( \int_0^{\sin(\theta)}\sqrt{1-r^2}r\,rd \right).
    \end{equation}
    Remarquez la présence d'un $r$ supplémentaire pour le jacobien.

    Notez que les coordonnées du point $P$ sont $(1,1)$.
\end{example}

En pratique, lors du passage en coordonnées polaires, le «$dxdy$» devient «$r\,drd\theta$».

\begin{example}
    On veut évaluer l'intégrale de la fonction $f(x,y)= x^2+y^2$ sur la région $V$ suivante :
    \[
    V=\{(x,y) \in \eR^2\,\vert\, x^2+y^2\leq 1,\, x>0,\, y>0\}.
    \]
    On peut faire le calcul directement,
    \[
    \int_{V}f(x,y)\, dV=\int_0^1\int_0^{\sqrt{1-x^2}}x^2+y^2\, dy\,dx=\int_0^1\left(x^2\sqrt{1-x^2} + \frac{(1-x^2)^{3/2}}{3}\right) dx
    \]
    mais c'est un peu ennuyeux. On peut simplifier beaucoup les calculs avec un changement de variables vers les coordonnées polaires. Dans ce cas, on sait bien que le difféomorphisme à utiliser est $\phi(r,\theta)=(r\cos \theta, r\sin\theta)$. Le jacobien  $J_{\phi}$ est
    \begin{equation}
     J_{\phi}(r, \theta)= \left\vert\begin{array}{cc}
    \cos \theta & \sin \theta \\
    -r\sin \theta  & r\cos \theta
    \end{array}\right\vert= r,
    \end{equation}
    qui est toujours positif. D'une part, la fonction $f$ peut s'écrire sous la forme $f(\phi(r,\theta))=r^2$ et d'autre part, $\phi^{-1}(V)=]0,1]\times]0, \pi/2[$. Par conséquent, la formule du changement de variables nous donne
    \[
    \int_{V}f(x,y)\, dV=\int_0^{\pi/2}\int_0^{1}r^3 dr\,d\theta=\int_0^{\pi/2}\frac{1}{4}\,d\theta=\frac{\pi}{8}.
    \]
\end{example}

\begin{example}
    Montrons comment intégrer la fonction $f(x,y)=\sqrt{1-x^2-y^2}$ sur le domaine délimité par la droite $y=x$ et le cercle $x^2+y^2=y$, représenté sur la figure~\ref{LabelFigQXyVaKD}. Pour trouver le centre et le rayon du cercle $x^2+y^2=y$, nous commençons par écrire $x^2+y^2-y=0$, et ensuite nous reformons le carré : $y^2-y=(y-\frac{ 1 }{2})^2-\frac{1}{ 4 }$.
    \newcommand{\CaptionFigQXyVaKD}{Passage en polaire pour intégrer sur un morceau de cercle.}
\input{auto/pictures_tex/Fig_QXyVaKD.pstricks}

    Le passage en polaire transforme les équations du bord du domaine en
    \begin{equation}
        \begin{aligned}[]
            \cos(\theta)&=\sin(\theta)\\
            r^2&=r\sin(\theta).
        \end{aligned}
    \end{equation}
    L'angle $\theta$ parcours donc $\mathopen] 0 , \pi/4 \mathclose[$, et le rayon, pour chacun de ces $\theta$ parcours $\mathopen] 0 , \sin(\theta) \mathclose[$. La fonction à intégrer se note maintenant $f(r,\theta)=\sqrt{1-r^2}$. Donc l'intégrale à calculer est
    \begin{equation}		\label{PgRapIntMultFubiniBoutCercle}
        \int_{0}^{\pi/4}\left( \int_0^{\sin(\theta)}\sqrt{1-r^2}r\,rd \right).
    \end{equation}
    Remarquez la présence d'un $r$ supplémentaire pour le jacobien.

    Notez que les coordonnées du point $P$ sont $(1,1)$.
\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Coordonnées cylindriques}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    On veut calculer le volume de la région $A$ définie par  l'intersection entre la boule unité et le cylindre qui a pour base un disque de rayon $1/2$ centré en $(0, 1/2)$
    \[
    A=\{(x,y,z) \in\eR^3 \,\vert\, x^2+y^2+z^1\leq 1\}\cap\{(x,y,z) \in \eR^3\,\vert\, x^2+(y-1/2)^2\leq 1/4\}.
    \]
    On peut décrire $A$ en coordonnées cylindriques
    \begin{equation}
      \begin{aligned}
        A=\Big\{(r,\theta,z) &\in ]0, +\infty[\times [-\pi,\pi[\times \eR\,\vert\,\\
    & -\pi/2<\theta<\pi, \, 0<r\leq \sin\theta, \, -\sqrt{1-r^2}\leq z\leq\sqrt{1-r^2} \Big\}.
      \end{aligned}
    \end{equation}
    Le jacobien de ce changement de variables,  $J_{cyl}$, est
    \begin{equation}
     J_{cyl}(r, \theta), z= \left\vert\begin{array}{ccc}
    \cos \theta & \sin \theta & 0\\
    -r\sin \theta  & r\cos \theta &0 \\
    0&0&
    \end{array}\right\vert= r,
    \end{equation}
    qui est toujours positif. Le volume de $A$ est donc
    \[
    \int_{\eR^3}\chi_{A}(x,y,z)\, dV=\int_{-\pi/2}^{\pi/2}\int_0^{\sin\theta}\int_{-\sqrt{1-r^2}}^{\sqrt{1-r^2}} r dz\,dr\,d\theta=\frac{2\pi}{8}+\frac{8}{9}.
    \]
\end{example}

\begin{example}[Volume d'un solide de révolution]
Soit $g:[a,b]\to\eR_+$ une fonction continue et positive. On dit que le solide $A$ décrit par
\[
A=\left\{(x,y,z)\in\eR^3\, \vert \, z\in[a,b], \,\sqrt{x^2+y^2}\leq g^2(z) \right\}
\]
est un solide de révolution. Afin de calculer son volume, on peut décrire $A$ en coordonnées cylindriques,
\[
A=\left\{(r,\theta,z) \in ]0, +\infty[\times [-\pi,\pi[\times \eR\,\vert\, a\leq z\leq b, \, 0<r^2\leq g^2(z) \right\}.
\]
Le jacobien de ce changement de variables est  $J_{cyl}=r$, comme dans l'exemple précédent. Le volume de $A$ est donc
\[
\int_{\eR^3}\chi_{A}(x,y,z)\, dV=\int_a^{b}\int_{-\pi}^{\pi}\int_{0}^{g(z)} r  \,dr\,d\theta\, dz=\int_a^{b} \pi g^2(z) \, dz.
\]
Cette formule peut être utilisée pour tout solide de révolution.
\end{example}

%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Coordonnées sphériques}
%///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Le calcul est un peu plus long :
\begin{equation}
    \begin{aligned}[]
        J(\rho,\theta,\varphi)&=\begin{vmatrix}
            \frac{ \partial x }{ \partial \rho }    &   \frac{ \partial x }{ \partial \theta }    &   \frac{ \partial x }{ \partial \varphi }    \\
            \frac{ \partial y }{ \partial \rho }    &   \frac{ \partial y }{ \partial \theta }    &   \frac{ \partial y }{ \partial \varphi }    \\
            \frac{ \partial z }{ \partial \rho }    &   \frac{ \partial z }{ \partial \theta }    &   \frac{ \partial z }{ \partial \varphi }
        \end{vmatrix}\\
        &=
        \begin{vmatrix}
            \sin\theta\cos\varphi    &   \rho\cos\theta\cos\varphi    &   -\rho\sin\theta\sin\varphi    \\
            \sin\theta\sin\varphi    &   \rho\cos\theta\sin\varphi    &   -\rho\sin\theta\cos\varphi    \\
            \cos\theta               &   -\rho\sin\theta              &   0
        \end{vmatrix}\\
        &=\rho^2\sin\theta.
    \end{aligned}
\end{equation}
Donc
\begin{equation}
    dx\,dy\,dz=\rho^2\sin(\theta)\,d\rho\,d\theta\,d\varphi.
\end{equation}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Coordonnées sphériques}
%---------------------------------------------------------------------------------------------------------------------------

\begin{example}
    On veut calculer le volume du cornet de glace  $A$
    \[
    A=\left\{(x,y,z)\in\eR^3\, \vert \, (x,y)\in \mathbb{S}^2, \,\sqrt{x^2+y^2}\leq z\leq \sqrt{1-x^2-y^2} \right\}.
    \]
    On peut décrire $A$ en coordonnées sphériques.
    \[
    A=\{(\rho,\theta,\phi) \in ]0, +\infty[\times [-\pi,\pi[\times [0,\pi[\,\vert\, 0<\phi\leq\pi/4, \, 0<\rho\leq 1 \}.
    \]
    Le jacobien de ce changement de variables  $J_{sph}$ est
    \begin{equation}
     J_{sph}(\rho, \theta, \phi)= \left\vert\begin{array}{ccc}
    \cos \theta \sin\phi & \sin \theta\sin\phi & \cos\phi\\
    -\rho\sin \theta\sin\phi  & \rho\cos \theta\sin\phi & 0 \\
    \rho\cos\theta\cos\phi&\rho\sin\theta\cos\phi& -\rho\sin\phi
    \end{array}\right\vert= \rho^2\sin\phi,
    \end{equation}
    Le volume de $A$ est donc
    \[
    \int_{\eR^3}\chi_{A}(x,y,z)\, dV=\int_{-\pi}^{\pi}\int_0^{\pi/4}\int_{0}^{1}\rho^2\sin\phi \,d\rho\,d\phi\,d\theta=\frac{2\pi}{3}\left(1-\frac{1}{\sqrt{2}}\right).
    \]
\end{example}

\begin{example}[Une petite faute à ne pas faire]
    Si nous voulons calculer le volume de la sphère de rayon $R$, nous écrivons donc
    \begin{equation}
        \int_0^Rdr\int_{0}^{2\pi}d\theta\int_0^{\pi}r^2 \sin(\phi)d\phi=4\pi R=\frac{ 4 }{ 3 }\pi R^3.
    \end{equation}
    Ici, la valeur absolue n'est pas importante parce que lorsque $\phi\in\mathopen] 0,\pi ,  \mathclose[$, le sinus de $\phi$ est positif.

    Des petits malins pourraient remarquer que le changement de variable \eqref{EqChmVarSpherique} est encore un paramétrage de $\eR^3$ si on intervertit le domaine des angles :
    \begin{equation}
        \begin{aligned}[]
            \theta&\colon 0 \to \pi\\
            \phi	&\colon 0\to 2\pi,
        \end{aligned}
    \end{equation}
    alors nous paramétrons encore parfaitement bien la sphère, mais hélas
    \begin{equation}		\label{EqVolumeIncorrectSphere}
        \int_0^Rdr\int_{0}^{\pi}d\theta\int_0^{2\pi}r^2 \sin(\phi)d\phi=0.
    \end{equation}
    Pourquoi ces «nouvelles» coordonnées sphériques sont-elles mauvaises ? Il y a que quand l'angle $\phi$ parcours $\mathopen] 0 , 2\pi \mathclose[$, son sinus n'est plus toujours positif, donc la \emph{valeur absolue} du jacobien n'est plus $r^2\sin(\phi)$, mais $r^2\sin(\phi)$ pour les $\phi$ entre $0$ et $\pi$, puis $-r^2\sin(\phi)$ pour $\phi$ entre $\pi$ et $2\pi$. Donc l'intégrale \eqref{EqVolumeIncorrectSphere} n'est pas correcte. Il faut la remplacer par
    \begin{equation}
        \int_0^Rdr\int_{0}^{\pi}d\theta\int_0^{\pi}r^2 \sin(\phi)d\phi- \int_0^Rdr\int_{0}^{\pi}d\theta\int_{\pi}^{2\pi}r^2 \sin(\phi)d\phi = \frac{ 4 }{ 3 }\pi R^3
    \end{equation}

\end{example}

%--------------------------------------------------------------------------------------------------------------------------- 
\subsection{Un autre système utile}
%---------------------------------------------------------------------------------------------------------------------------

Un changement de variables que l'on voit assez souvent est
\begin{subequations}
    \begin{numcases}{}
        u=x+y\\
        v=x-y.
    \end{numcases}
\end{subequations}
Afin de calculer son jacobien, il faut d'abord exprimer $x$ et $y$ en fonctions de $u$ et $v$ :
\begin{subequations}
    \begin{numcases}{}
        x=(u+v)/2\\
        y=(u-v)/2.
    \end{numcases}
\end{subequations}
La matrice jacobienne est
\begin{equation}
    \begin{pmatrix}
        \frac{ \partial x }{ \partial u }    &   \frac{ \partial x }{ \partial v }    \\
        \frac{ \partial y }{ \partial u }    &   \frac{ \partial y }{ \partial v }
    \end{pmatrix}=
    \begin{pmatrix}
        \frac{ 1 }{2}    &   \frac{ 1 }{2}    \\
        \frac{ 1 }{2}    &   -\frac{ 1 }{2}
    \end{pmatrix}.
\end{equation}
Le déterminant vaut $-\frac{1}{ 2 }$. Nous avons donc
\begin{equation}
    dxdy=\frac{ 1 }{2}dudv.
\end{equation}
Nous insistons sur le fait que c'est $\frac{ 1 }{2}$ et non $-\frac{ 1 }{2}$ qui intervient parce que que la formule du changement de variable demande d'introduire la \emph{valeur absolue} du jacobien.

\begin{example}
    Calculer l'intégrale de la fonction $f(x,y)=x^2-y^2$ sur le domaine représenté sur la figure~\ref{LabelFigVWFLooPSrOqz}. % From file VWFLooPSrOqz
\newcommand{\CaptionFigVWFLooPSrOqz}{Un domaine qui s'écrit étonnament bien avec un bon changement de coordonnées.}
\input{auto/pictures_tex/Fig_VWFLooPSrOqz.pstricks}

    Les droites qui délimitent le domaine d'intégration sont
    \begin{equation}
        \begin{aligned}[]
            y&=-x+2\\
            y&=x-2\\
            y&=x\\
            y&=-x
        \end{aligned}
    \end{equation}
    Le domaine est donc donné par les équations
    \begin{subequations}
        \begin{numcases}{}
            y+x<2\\
            y-x>-2\\
            y-x<0 \\
            y+x>0.
        \end{numcases}
    \end{subequations}
    En utilisant le changement de variables $u=x+y$, $v=x-y$ nous trouvons le domaine $0<u<2$, $0<v<2$. En ce qui concerne la fonction, $f(x,y)=(x+y)(x-y)$ et par conséquent
    \begin{equation}
        f(u,v)=uv.
    \end{equation}
    L'intégrale à calculer est simplement
    \begin{equation}
        \int_0^2\int_0^2 uv\,dudv=\int_0^2 u\,du\left[ \frac{ v^2 }{ 2 } \right]_0^2=2\int_0^2u\,du=4.
    \end{equation}
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{Aire d'une surface de révolution}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Soit $\gamma$ une courbe dans le plan $xy$, paramétrée par
\begin{equation}
    \gamma(u)=\begin{pmatrix}
        x(u)    \\
        y(u)    \\
        0
    \end{pmatrix}
\end{equation}
avec $u\in\mathopen[ a , b \mathclose]$. Nous supposons que la courbe est toujours positive, c'est-à-dire $y(u)>0$ pour tout $u$.

Nous voulons considérer la surface obtenue en effectuant une rotation de cette ligne autour de l'axe $X$. Chaque point de la courbe va parcourir un cercle de rayon $y(u)$ dans le plan $YX$ et centré en $(x(u),0,0)$. La surface est donc donnée par
\begin{equation}
    \varphi(u,\theta)=\begin{pmatrix}
        x(u)    \\
        y(u)\cos\theta    \\
        y(u)\sin\theta
    \end{pmatrix}
\end{equation}
avec $(u,\theta)\in\mathopen[ a , b \mathclose]\times \mathopen[ 0 , 2\pi \mathclose]$. Notez que la courbe de départ correspond à $\theta=0$.

Les vecteurs tangents à la surface pour ce paramétrage sont
\begin{equation}
    \begin{aligned}[]
        T_u&=\frac{ \partial \varphi }{ \partial u }=\begin{pmatrix}
            x'(u)    \\
            y'(u)\cos\theta    \\
            y'(u)\sin\theta
        \end{pmatrix}&
        T_{\theta}&=\frac{ \partial \varphi }{ \partial \theta }=\begin{pmatrix}
            0    \\
            -y(u)\sin\theta    \\
            y(u)\cos\theta
        \end{pmatrix}.
    \end{aligned}
\end{equation}
Le produit vectoriel de ces deux vecteurs vaut
\begin{equation}
    \begin{aligned}[]
        T_u\times T_{\theta}&=\begin{vmatrix}
            e_x    &   e_y    &   e_z    \\
            x'    &   y'\cos\theta    &   y'\sin\theta    \\
            0    &   -y\sin\theta    &   y\cos\theta
        \end{vmatrix}\\
        &=y'(u)y(u)\,e_x-x'(u)y(u)\cos\theta\, e_y+x'(u)y(u)\sin\theta\, e_z.
    \end{aligned}
\end{equation}
En ce qui concerne la norme :
\begin{equation}
    dS=\| T_u\times T_{\theta} \|=\sqrt{(y'y)^2+(x'y)^2}=| y(u) |\sqrt{y'(u)^2+x'(u)^2}.
\end{equation}
Étant donné que nous avons supposé que $y(u)>0$, nous pouvons supprimer les valeurs absolues, et l'aire de la surface de révolution devient :
\begin{equation}
    \begin{aligned}[]
        Aire(S)&=\int_0^{2\pi}d\theta\int_a^b y(u)\sqrt{x'(u)^2+y'(u)^2}du\\
        &=2\pi\int_a^b y(u)\sqrt{x'(u)^2+y'(u)^2}du.
    \end{aligned}
\end{equation}

\begin{example}     \label{EXooZCLXooVmXQgY}
    Calculons la surface du cône de révolution de rayon (à la base) $R$ et de hauteur $h$. La courbe de départ est le segment droite qui part de $(0,0)$ et qui termine en $(R,h)$ de la figure~\ref{LabelFigYHJYooTEXLLn}. % From file YHJYooTEXLLn
\newcommand{\CaptionFigYHJYooTEXLLn}{En faisant tourner cette droite autour de l'axe $X$, nous obtenons un cône.}
\input{auto/pictures_tex/Fig_YHJYooTEXLLn.pstricks}

    Ce segment peut être paramétré par
    \begin{equation}
        \gamma(u)=\begin{pmatrix}
            Ru    \\
            hu    \\
            0
        \end{pmatrix}
    \end{equation}
    avec $u\in\mathopen[ 0 , 1 \mathclose]$. Cela donne $x(u)=Ru$, $y(u)=hu$ et par conséquent
    \begin{equation}
        Aire=2\pi\int_0^1hu\sqrt{R^2+h^2}=\pi h\sqrt{R^2+h^2}.
    \end{equation}
    Ce résultat peut aussi être exprimé en fonction de l'angle, grâce à la formule \eqref{EQooEKZEooFeNImX}. En sachant que $h=\sqrt{h^2+R^2}\sin(\alpha)$, nous trouvons
    \begin{equation}
        Aire=\pi(R^2+h^2)\sin(\alpha).
    \end{equation}

\end{example}

\begin{example}
    Calculons la surface latérale du tore obtenu par révolution du cercle de la figure ~\ref{LabelFigROAOooPgUZIt}. % From file ROAOooPgUZIt
\newcommand{\CaptionFigROAOooPgUZIt}{Si nous tournons ce cercle autour de l'axe $X$, nous obtenons un tore de rayon «externe» $a$ et de rayon «interne» $R$.}
\input{auto/pictures_tex/Fig_ROAOooPgUZIt.pstricks}

    Le chemin qui détermine le cercle de départ est
    \begin{equation}
        \gamma(u)=\begin{pmatrix}
            R\cos(u)    \\
            a+R\sin(u)    \\
            0
        \end{pmatrix},
    \end{equation}
    c'est-à-dire $x(u)=R\cos(u)$, $y(u)=a+R\sin(u)$ avec $u\in\mathopen[ 0 , 2\pi \mathclose]$. Nous avons donc l'aire
    \begin{equation}
        \begin{aligned}[]
            Aire&=2\pi\int_0^{2\pi}\big( a+R\sin(u) \big)R\,du\\
            &=2\pi R\big( 2\pi a+R[-\cos(u)]_0^{2\pi} \big)\\
            &=4\pi^2aR.
        \end{aligned}
    \end{equation}
\end{example}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
\section{La fonction sinus cardinal, intégrale de Dirichlet}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{definition}
    La fonction \defe{sinus cardinal}{sinus cardinal} est
    \begin{equation}
        f(t)=\begin{cases}
            1    &   \text{si } t=0 \\
            \frac{ \sin(t) }{ t }    &    \text{sinon. }
        \end{cases}
    \end{equation}
\end{definition}
Elle sert à plein de choses. Entre autres, le lemme \ref{LEMooEEWSooZwLSAP} montrera que la fonction \( x\mapsto | \sin(x)/x |\) a une intégrale sur \( \eR\) qui vaut \( \infty\). Cela nous permettra de donner un exemple d'une fonction dans \( L^1(\eR)\) dont la transformée de Fourier n'est pas dans \( L^1(\eR)\) (lemme \ref{LEMooROPHooOSguhN}).

\begin{normaltext}
    Le but que nous nous fixons maintenant est de prouver que
    \begin{equation}
        \int_{0}^{\infty}\frac{ \sin(t) }{ t }dt=\frac{ \pi }{2}.
    \end{equation}

    Un adage dit que si un théorème est trop long, c'est qu'il n'a pas assez de lemmes. Nous allons faire plein de lemmes.
\end{normaltext}

\begin{lemma}
    La fonction sinus cardinal est continue.
\end{lemma}

\begin{proof}
    Elle est continue en zéro parce que le lemme \ref{LEMooZYNEooYkwsWD} nous donne
    \begin{equation}
        \lim_{t\to 0}\frac{ \sin(t) }{ t }=1.
    \end{equation}
\end{proof}

Nous commençons par une mauvaise nouvelle.
\begin{lemma}[\cite{MonCerveau}]           \label{LEMooEEWSooZwLSAP}
    Nous avons
    \begin{equation}
        \int_{\eR}| \frac{ \sin(x) }{ x } |dx=\infty.
    \end{equation}
\end{lemma}

\begin{proof}
    Soit \( \delta>0\) tel que sur l'intervalle \( \mathopen[ \frac{ \pi }{2}-\delta , \frac{ \pi }{ 2 }+\delta \mathclose]\), nous ayons \( \sin(x)>0.9\)\footnote{Ça existe par une astucieuse combinaison du théorème \ref{ThoValInter} des valeurs intermédiaires, de la valeur remarquable \( \sin(\pi/2)=1\) (de \eqref{SUBEQSooBTNPooSvCAHO}) et du fait que \( \sin\) est continue (proposition \ref{PROPooZXPVooBjONka}).} .

    Les intervalles \( I_k\mathopen[ \frac{ \pi }{2}-\delta+2k\pi , \frac{ \pi }{2}+\delta+2k\pi \mathclose]\) sont disjoints et la fonction que nous intégrons est partout positive. Nous découpons
    \begin{equation}
        \eR=C+\bigcup_{k=0}^{\infty}\mathopen[ \frac{ \pi }{2}-\delta+2k\pi , \frac{ \pi }{2}+\delta+2k\pi \mathclose]
    \end{equation}
    où \( C\) est le complémentaire qu'il faut pour faire \( \eR\).

    La \( \sigma\)-additivité de l'intégrale de Lebesgue (proposition \ref{PROPooDWYNooWKJmEV}) nous indique que
    \begin{equation}
        \int_{\eR}\big| \frac{ \sin(x) }{ x } \big|= \int_C\big| \frac{ \sin(x) }{ x } \big|+  \sum_{k=0}^{\infty}\int_{I_k}\big| \frac{ \sin(x) }{ x } \big|dx
    \end{equation}
    Vu que tous les termes sont positifs, nous obtenons une majoration en en supprimant un. Allons-y :
    \begin{subequations}        \label{SUBEQSooSRAYooEOBwiC}
        \begin{align}
            \int_{\eR}\big| \frac{ \sin(x) }{ x } \big|dx&\geq \sum_{k=0}^{\infty}\int_{I_k}\big| \frac{ \sin(x) }{ x } \big|dx\\
            &\geq\sum_{k=0}^{\infty}0.9\int_{I_k}\frac{ 1 }{ x }dx\\
            &\geq 0.9\sum_{k=0}^{\infty}2\delta\frac{1}{ \frac{ \pi }{2}-\delta+2k\pi }     \label{SUBEQooKMURooVuIpCo}\\
            &\geq 1.8\delta\sum_{k=0}^{\infty}\frac{1}{ 2\pi(k+1) }     \label{SUBEQooJCQOooYqUCps}\\
            &=\frac{ 1.8\delta }{ 2\pi }\sum_{k=0}^{\infty}\frac{1}{ k+1 }.
        \end{align}
    \end{subequations}
    Justifications :
    \begin{itemize}
        \item Pour \eqref{SUBEQooKMURooVuIpCo}, nous avons majoré \( \frac{1}{ x }\) par \( \frac{1}{ \frac{ \pi }{ 2 }+\delta+2k\pi }\) sur \( I_k\).
        \item Pour \eqref{SUBEQooJCQOooYqUCps}, nous avons dit que \( \frac{ \pi }{2}+\delta<2\pi\).
    \end{itemize}
    La dernière somme dans \eqref{SUBEQSooSRAYooEOBwiC} diverge.

    Donc la fonction sinus cardinal n'est pas dans \( L^1(\eR)\).
\end{proof}

La mauvaise nouvelle suivante en est un corolaire immédiat.
\begin{lemma}       \label{LEMooBEQRooHaugKj}
    La fonction \( t\mapsto \frac{ \sin(t) }{ t }\) n'est pas intégrable sur \( \mathopen[ 0 , \infty \mathclose[\) au sens de Lebesgue.
\end{lemma}

\begin{proof}
    Le lemme \ref{LEMooEEWSooZwLSAP} nous dit que \( \int_0^{\infty}| f |=\infty\). Dans ce cas, \( \int_0^{\infty}f\) n'existe pas par le lemme \ref{LEMooMWKTooIKomSw}.
\end{proof}

Donc l'intégrale \( \int_0^{\infty}\frac{ \sin(t) }{ t }dt\) n'existe pas parce que la définition de l'intégrale de Lebesgue ne permet pas de profiter des compensations qui arrivent entre les valeurs positives et négatives.

Nous définissons donc
\begin{equation}        \label{EQooWAQLooTFOPbl}
    \int_0^{\infty}\frac{ \sin(t) }{ t }dt=\lim_{b\to \infty} \int_0^b\frac{ \sin(t) }{ t }dt.
\end{equation}
Pour chaque \( b\), l'intégrale existe sans problèmes (fonction continue sur le compact \( \mathopen[ 0 , b \mathclose]\)), et les compensations se font. Il n'est pas pas sans espoir que la limite \eqref{EQooWAQLooTFOPbl} existe et valle un nombre fini.

\begin{lemma}[\cite{BIBooCFXJooWrArNT}]     \label{LEMooTFVZooRAmjUN}
    La limite
    \begin{equation}
        \lim_{b\to \infty}\int_0^b\frac{ \sin(t) }{ t }dt
    \end{equation}
    existe dans \( \eR\).
\end{lemma}

\begin{proof}
    En plusieurs parties.
    \begin{subproof}
        \item[Découpage]

            Nous découpons l'intervalle \( \mathopen[ 0 , b \mathclose]\) en morceaux du type \( \mathopen[ k\pi , (k+1)\pi \mathclose]\) et un morceau restant lorsque \( b\) n'est pas un multiple de \( \pi\) :
            \begin{equation}
                \mathopen[ 0 , b \mathclose]=\bigcup_{k=0}^{N(b)-1}\mathopen[ k\pi , (k+1)\pi \mathclose]\cup\mathopen[ N(b)\pi , b \mathclose]
            \end{equation}
            où \( N(b)\) est un entier bien choisi\quext{Il me semble que le traitement de ce terme manque dans \cite{BIBooCFXJooWrArNT}.}. En tout cas \( \lim_{b\to \infty}N(b)=\infty\).

        \item[Majoration 1]

            Pour chaque \( b\in \eR^+\) nous avons
            \begin{equation}
                \int_0^b\frac{ \sin(t) }{ t }dt=\sum_{k=0}^{N(b)}\int_{\mathopen\big[ k\pi , (k+1)\pi \mathclose\big]}\frac{ \sin(t) }{ t }dt+\int_{\mathopen[ \big( N(b)+1 \big)\pi , b \mathclose]}\frac{ \sin(t) }{ t }
            \end{equation}
            Dans le dernier terme, nous majorons \( | \sin(t) |\leq 1\) et \( \frac{1}{ t }\leq \big( N(b)+1 \big)\pi\). Cela donne
            \begin{equation}
                \left|  \int_{\mathopen\big[ \big(N(b)+1\big)\pi  , b \mathclose\big]}\frac{ \sin(t) }{ t } \right|\leq \frac{ b-\big( N(b)+1 \big)\pi }{ \big( N(b)+1 \big)\pi }\leq \frac{1}{ N(b)+1 }
            \end{equation}
            où nous avons encore majoré \( b\leq \big( N(b)+2 \big)\pi\).

        \item[Majoration 2]

            En ce qui concerne les autres termes, sur l'intervalle \( \mathopen[ k\pi , (k+1)\pi \mathclose]\), nous avons \( \sin(t)=(-1)^k| \sin(t) |\). Nous avons alors
            \begin{equation}        \label{EQooHZPRooFuwRWQ}
                \int_0^b\frac{ \sin(t) }{ t }dt=\sum_{k=0}^{N(b)}(-1)^k\int_{k\pi}^{(k+1)\pi}\frac{ | \sin(t) | }{ t }dt+\alpha(b)
            \end{equation}
            où \( | \alpha(b) |\leq \frac{1}{ N(b)+1 }\); l'important est que \( \lim_{b\to \infty}\alpha(b)=0\).
            
        \item[Une suite alternée]

            L'inégalité \eqref{EQooHZPRooFuwRWQ} nous incite à étudier la série \( \sum_{k=0}^{\infty}(-1)^ka_k\) en ayant posé
            \begin{equation}
                a_k=\int_{k\pi}^{(k+1)\pi}\frac{ | \sin(t) | }{ t }dt.
            \end{equation}
            Nous montrons à présent que la suite \( (a_k)\) vérifie les conditions du critère des séries alternées \ref{THOooOHANooHYfkII}.

            D'abord, \( a_{k+1}\leq a_k\). En effet en utilisant le changement de variables\footnote{Le théorème \ref{THOooUMIWooZUtUSg} est toujours bon à citer.} \( u=t-\pi\),
            \begin{equation}
                a_{k+1}=\int_{(k+1)\pi}^{(k+2)\pi}\frac{ | \sin(t) | }{ t }dt=\int_{k\pi}^{(k+1)\pi}\frac{ | \sin(u+\pi) | }{ u+\pi }du=\int_{k\pi}^{(k+1)\pi}\frac{ | \sin(u) | }{ u+\pi }du<a_k.
            \end{equation}
            Nous avons utilisé le fait que \( | \sin(u+\pi) |=| \sin(u) |\) pour tout \( u\).

            De plus, vu que \( | \sin(t) |\leq 1\) et que \( t\in\mathopen[ k\pi , (k+1)\pi \mathclose]\), nous avons
            \begin{equation}
                \int_{k\pi}^{(k+1)\pi}\frac{ | \sin(t) | }{ t }\leq \int_{k\pi}^{(k+1)\pi}\frac{1}{ k\pi }=\frac{ (k+1)\pi-k\pi }{ k\pi }=\frac{1}{ k }.
            \end{equation}
            Donc \( a_k\leq\frac{1}{ k }\to 0\).

            Le critère des séries alternées \ref{THOooOHANooHYfkII} nous dit que
            \begin{equation}
                \sum_{k=0}^{\infty}(-1)^ka_k<\infty.
            \end{equation}

        \item[Conclusion]
            Nous repartons de \eqref{EQooHZPRooFuwRWQ} :
            \begin{equation} 
                \int_0^b\frac{ \sin(t) }{ t }dt=\sum_{k=0}^{N(b)}(-1)^k\int_{k\pi}^{(k+1)\pi}\frac{ | \sin(t) | }{ t }dt+\alpha(b).
            \end{equation}
            Cette égalité est valable pour tout \( b\in \eR^+\). Le passage à la limite \( b\to 0\) à droite donne un nombre fini; donc à gauche aussi, et nous avons prouvé que
            \begin{equation}
                \lim_{b\to\infty} \int_0^b\frac{ \sin(t) }{ t }dt<\infty.
            \end{equation}
    \end{subproof}
\end{proof}
Notre tache n'est donc pas sans espoir. Au moins l'intégrale que nous cherchons à évaluer est finie.

\begin{lemma}       \label{LEMooARPIooDPSGwR}
    Soit \( x\in \mathopen] 0 , \infty \mathclose[\). L'intégrale
    \begin{equation}
        F(x)=\int_0^{\infty} e^{-tx}\frac{ \sin(t) }{ t }dt
    \end{equation}
    existe au sens de Lebesgue usuel.
\end{lemma}

\begin{proof}
    Vu qu'en \( t=0\) nous avons \( \frac{ \sin(t) }{ t }=1\), il n'y a pas de problèmes de ce côté. Lorsque \( t>1\) nous avons la majoration
    \begin{equation}
        |  e^{-tx}\frac{ \sin(t) }{ t } |\leq |  e^{-tx} |.
    \end{equation}
    Lorsque \( t\) est assez grand, le lemme \ref{LEMooNYFVooXjFShk} nous donne aussi la majoration
    \begin{equation}
        |  e^{-tx} |\leq \frac{1}{ t^2 }.
    \end{equation}
    La proposition \ref{PropBKNooPDIPUc}\ref{ITEMooJFSXooHmgmEj} implique que \( \int_1^{\infty}\frac{1}{ t^2 }<\infty\). Et les majorations font que la proposition \ref{PROPooGTMVooPHcrRl} nous donne le résultat.
\end{proof}

\begin{lemma}[\cite{BIBooCFXJooWrArNT}]     \label{LEMooRDCSooBrWmep}
    Il existe une constante \( C\in \eR\) telle que
    \begin{equation}
        I(x)=\int_0^{\infty} e^{-tx}\frac{ \sin(t) }{ t }dt=-\arctan(x)+C
    \end{equation}
    pour tout \( x>0\).
\end{lemma}

\begin{proof}
    En permutant dérivée et intégrale, nous allons prouver que \( I'(x)=-\frac{1}{ 1+x^2 }\).

    \begin{subproof}
        \item[Permuter]
            Nous posons
            \begin{equation}
                f(x,t)= e^{-tx}\frac{ \sin(t) }{ t },
            \end{equation}
            et nous vérifions les hypothèses du théorème \ref{ThoMWpRKYp}.

            \begin{enumerate}
                \item
                    Pour chaque \( x>0\) fixé, la fonction \( t\mapsto f(x,t)\) est intégrable sur \( \mathopen[ 0 , \infty \mathclose[\), c'est le lemme \ref{LEMooARPIooDPSGwR}.
                    \item
                        Pour \( t>0\) fixé, la fonction \( x\mapsto f(x,t)\) est dérivable.
                    \item
                        Nous avons la dérivée partielle
                        \begin{equation}
                            \frac{ \partial f }{ \partial x }(x,t)=- e^{-tx}\sin(t)
                        \end{equation}
                        qui vérifie
                        \begin{equation}
                            | \frac{ \partial f }{ \partial x }(x,t) |\leq  e^{-tx},
                        \end{equation}
                        alors que la fonction \( t\mapsto  e^{-tx}\) est intégrable sur \( \mathopen[ 0 , \infty \mathclose[\).
            \end{enumerate}
            Nous pouvons donc dériver sous l'intégrale et obtenir
            \begin{equation}
                F'(x)=-\int_0^{\infty} e^{-xt}\sin(t)dt.
            \end{equation}
        \item[Quelques intégrations par partie]
            Nous posons
            \begin{equation}
                J(x)=\int_0^{\infty} e^{-xt}\sin(t)dt,
            \end{equation}
            et nous allons la faire par parties\footnote{Proposition \ref{PROPooRLFIooQHnyJY}.}, en deux fois.

            D'abord en posant \( u= e^{-xt}\) et \( v'=\sin(t)\) nous avons
            \begin{equation}
                J(x)=\left[ - e^{-xt}\cos(t) \right]_{t=0}^{t=\infty}-\int_0^{\infty}(-)x e^{-xt}(-)\cos(t)dt=1-x\int_0^{\infty} e^{-xt}\cos(t)dt.
            \end{equation}
            Nous faisons l'intégrale encore par parties en posant \( u= e^{-xt}\) et \( v'=\cos(t)\) :
            \begin{equation}
                \int_0^{\infty} e^{-xt}\cos(t)dt=\left[  e^{-xt}\sin(t) \right]_0^{\infty}-\int_0^{\infty}(-) e^{-xt}\sin(t)dt=J(x).
            \end{equation}
            Donc
            \begin{subequations}
                \begin{align}
                    J(x)&=1-x\int_0^{\infty} e^{-xt}\cos(t)dt\\
                    &=1-x\int_0^{\infty} e^{-xt}\cos(t)dt\\
                    &=1-x\Big( x\underbrace{\int_0^{\infty} e^{-xt}\sin(t)dt}_{J(x)} \Big)\\
                    &=1-x^2J(x).
                \end{align}
            \end{subequations}
            Voila qui prouve que \( J(x)=\frac{1}{ 1+x^2 }\), et donc que
            \begin{equation}
                F'(x)=-J(x)=-\frac{1}{ 1+x^2 }.
            \end{equation}
            
        \item[Et enfin]

            Le théorème \ref{THOooUSVGooOAnCvC}\ref{ITEMooMNHLooOVhIIb} nous dit que la dérivée de la fonction \( \arctan\) est précisément \( 1/(1+x)\). Donc \( F\) et \( \arctan\) ont la même dérivée (au signe près). Donc il existe \( C\in \eR\) tel que
            \begin{equation}
                F(x)=-\arctan(x)+C
            \end{equation}
            pour tout \( x>0\).

    \end{subproof}
\end{proof}

\begin{lemma}       \label{LEMooEOYHooVIMCCa}
    Nous avons
    \begin{equation}
        F(x)=\int_0^{\infty} e^{-xt}\frac{ \sin(t) }{ t }dt=-\arctan(x)+\frac{ \pi }{2}
    \end{equation}
    pour tout \( x>0\).
\end{lemma}

\begin{proof}
    Le but de ce lemme est de fixer la constante laissée arbitraire dans le lemme \ref{LEMooRDCSooBrWmep}. Nous savons qu'il existe \( C\in \eR\) tel que
    \begin{equation}        \label{EQooTZGXooUxfAjT}
        F(x)=-\arctan(x)+C.
    \end{equation}
    Le but est de prendre la limite \( x\to\infty\) des deux côtés.

    Par le lemme \ref{LEMooSFALooVRBdNb}, nous avons \( | \frac{ \sin(t) }{ t } |\leq 1\) sur \( \mathopen[ 0 , \infty \mathclose[\). Donc
    \begin{equation}
        F(x)\leq \int_0^{\infty} e^{-xt}dt=\left[ -\frac{1}{ x } e^{-xt} \right]_{t=0}^{t=\infty}=\frac{1}{ x }.
    \end{equation}
    Vu que \( F(x)\leq \frac{1}{ x }\) pour tout \( x\), nous avons certainement \( \lim_{x\to \infty} F(x)=0\).

    D'autre part,
    \begin{equation}
        \lim_{x\to \infty} \arctan(x)=\frac{ \pi }{2}.
    \end{equation}
    
    En passant à la limite dans \eqref{EQooTZGXooUxfAjT}, nous avons
    \begin{equation}
        0=-\frac{ \pi }{2}+C,
    \end{equation}
    et donc \( C=\pi/2\).
\end{proof}

Nous avons maintenant la formule
\begin{equation}
    F(x)=\int_0^{\infty} e^{-tx}\frac{ \sin(t) }{ t }dt=-\arctan(x)+\frac{ \pi }{2}
\end{equation}
qui est valable pour tout \( x>0\).

Notre but sera de prendre la limite \( x\to 0\) des deux côtés. Vu que \( \arctan\) est continue, le membre de droite ne pose pas de problèmes et donne \( \pi/2\). Pour le membre de gauche, il faut encore permuter une limite et une intégrale.

Pour la suite, nous allons étudier\cite{BIBooCFXJooWrArNT}
\begin{equation}
    \int_0^{\infty}(1- e^{-xt})\frac{ \sin(t) }{ t }dt.
\end{equation}
Cette intégrale n'existe pas au sens de Lebesgue et est définie par
\begin{equation}
    L(x)=\lim_{b\to \infty} \int_0^b(1- e^{-xt})\frac{ \sin(t) }{ t }dt.
\end{equation}
Rien n'indique cependant pour l'instant que cette limite existe.

\begin{lemma}[\cite{BIBooCFXJooWrArNT}]     \label{LEMooZGODooLaBuHo}
    Soit \( x>0\). Nous posons
    \begin{equation}        \label{EQooJXWMooRbbCtt}
        L_k(x)=\int_{k\pi}^{(k+1)\pi}(1- e^{-tx})\frac{ | \sin(t) | }{ t }dt.
    \end{equation}
    La suite \( (L_k(x))_{k\in \eN}\) satisfait le critère des séries alternées\footnote{Théorème \ref{THOooOHANooHYfkII}.}, c'est-à-dire que cette suite est positive, décroissante à limite nulle.
\end{lemma}

\begin{proof}
    Notons que chacune des intégrales \( L_k(x)\) est sans problèmes : fonction continue sur un compact. Trois éléments à prouver.
    \begin{subproof}
        \item[Positive]
            Vu que dans toute notre histoire, \( x,t>0\), nous avons \( 1- e^{-tx}>0\) et donc toute la fonction intégrée est positive.
        \item[Tend vers zéro]
            Vu que \( 1- e^{-tx}<1\), nous avons
            \begin{equation}        \label{EQooCGAQooDSvbln}
                L_k(x)\leq \int_{k\pi}^{(k+1)\pi}\frac{1}{ t }dt\leq \pi\frac{1}{ k\pi }=\frac{1}{ k }.
            \end{equation}
            Donc \( \lim_{k\to \infty} L_k(x)=0\).
        \item[Décroissante]
            Nous devons à présent prouver que \( L_k(x)\) est décroissante en \( k\) lorsque \( x\) est fixé.

            Nous avons \( L_{k+1}(0)=L_k(0)\) pour tout \( k\). Nous allons montrer que \( L'_{k+1}(x)<L'_k(x)\) pour tout \( x>0\). De cette façon nous aurons bien \( L_{k+1}(x)<L_k(x)\) pour tout \( k\) et \( x\).

            En permutant (encore) intégrale et dérivée,
            \begin{subequations}
                \begin{align}
                    L_{k+1}'(x)&=\int_{(k+1)\pi}^{(k+2)\pi} e^{-tx}| \sin(t) |dt        \label{EQooAGGCooSoPHnz}\\
                    &=\int_{k\pi}^{(k+1)\pi} e^{-(u+\pi)x}| \sin(u+\pi) |       \label{SUBEQooEWZSooQtZBYI}\\
                    &= e^{-\pi x}\int_{k\pi}^{(k+1)\pi} e^{-ux}| \sin(u) |du    \label{SUBEQooYGDQooLWqrvg}\\
                    &= e^{-\pi x}L_k'(x).
                \end{align}
            \end{subequations}
            Justifications :
            \begin{itemize}
                \item Pour \eqref{EQooAGGCooSoPHnz}, permuter dérivée et intégrale; je ne donne pas tout le détail. Ça a déjà été fait.
                \item Pour \eqref{SUBEQooEWZSooQtZBYI}, nous avons fait le changement de variables \( u=t-\pi\).
                \item Pour \eqref{SUBEQooYGDQooLWqrvg}, nous avons utilisé le fait que \( | \sin(u+\pi) |=| \sin(u) |\) ainsi que \(  e^{-(u+\pi)x}= e^{-ux} e^{-\pi x}\) par \eqref{EQooEWIHooDRAQGR}.
            \end{itemize}
            Nous avons donc prouvé que
            \begin{equation}
                L_{k+1}'(x)= e^{-\pi x}L'_{k}(x)<L_k'(x).
            \end{equation}
    \end{subproof}
\end{proof}

\begin{lemma}       \label{LEMooSWFDooGLfwoD}
    Pour chaque \( x>0\), nous avons la limite
    \begin{equation}
        \lim_{b\to \infty} \int_0^{b}(1- e^{-tx})\frac{ \sin(t) }{ t }dt=\sum_{k=0}^{\infty}(-1)^kL_k(x)<\infty.
    \end{equation}
\end{lemma}

\begin{proof}
    Nous fixons (provisoirement) \( b\) et nous découpons l'intervalle d'intégration comme
    \begin{equation}
        \mathopen[ 0 , b \mathclose]=\bigcup_{k=1}^N\mathopen[ k\pi , (k+1)\pi \mathclose]\cup\mathopen\big[ (N+1)\pi  , b \mathclose\big]
    \end{equation}
    où \( N\) est une fonction de \( b\); quelque chose comme \( N(b)\) est le plus grand entier tel que \(\big( N(b)+1 \big)\pi\leq \pi\). Sur chacun des intervalles nous avons \( \sin(t)=(-1)^k| \sin(t) |\). Nous avons donc
    \begin{equation}       \label{EQooGBEDooSeuwMN}
        \begin{aligned}[]
            \int_0^b(1- e^{-tx})\frac{ \sin(t) }{ t }dt&=\sum_{k=0}^{N(b)}(-1)^k\int_{k\pi}^{(k+1)\pi}(1- e^{-tx})\frac{ | \sin(t) | }{ t }dt\\
                &\quad+\int_{(N+1)\pi}^b(1- e^{-tx})\frac{ \sin(t) }{ t }dt
        \end{aligned}
    \end{equation}
    Le premier terme est \( \sum_{k=0}^{N(b)}L_k(x)\), dont nous savons que la limite \( b\to \infty\) existe parce que \( L_k(x)\) vérifie le critère des séries alternées (lemme \ref{LEMooZGODooLaBuHo}). En ce qui concerne le second terme,
    \begin{equation}
        \big|\int_{(N+1)\pi}^b (1- e^{-tx})\frac{ \sin(t) }{ t }\big| <  \frac{  b-( N+1 )\pi    }{ ( N+1 )\pi }<\frac{1}{ N(b)+1 }.
    \end{equation}
    La dernière inégalité est le fait que \( N(b)\) est choisi pour avoir \( b-\big( N(b)+1 \big)\pi<\pi\).

            Les deux termes de \eqref{EQooGBEDooSeuwMN} ont donc une limite lorsque \( b\to \infty\). Nous pouvons donc passer à la limite en sommant les deux limites :
            \begin{equation}
                \int_0^{\infty}(1- e^{-tx})\frac{ \sin(t) }{ t }dt=\sum_{k=0}^{\infty}(-1)^kL_k(x).
            \end{equation}
            Cette égalité est valable pour chaque \( x>0\). 
            
            Le fait que la limite soit finie est dans le critère des séries alternées. Pour chaque \( x\), la suite \( L_k(x)\) vérifie ce critère par le lemme \ref{LEMooZGODooLaBuHo}.
\end{proof}

\begin{lemma}       \label{LEMooNZVSooDbZCZx}
    Nous avons
    \begin{equation}
        \lim_{x\to 0^+} \int_0^{\infty}(1- e^{-tx})\frac{ \sin(t) }{ t }dt=0.
    \end{equation}
\end{lemma}

\begin{proof}
    La définition de l'intégrale ainsi que le lemme \ref{LEMooSWFDooGLfwoD} nous ont déjà donné
    \begin{equation}
        \int_0^{\infty}(1- e^{-tx})\frac{ \sin(t) }{ t }dt=\lim_{b\to \infty} \int_0^b(1- e^{-tx})\frac{ \sin(t) }{ t }dt=\sum_{k=0}^{\infty}(-1)^kL_k(x)
    \end{equation}
    ainsi que l'assurance que le tout est un nombre réel fini\footnote{De toutes façons, il n'existe pas de nombres réels infinis, mais vous voyez ce que je veux dire.}.

    \begin{subproof}
        \item[Majoration pour la série alternée]

            Nous majorons un peu. Pour \( x>0\) et \( N\in \eN\) nous avons
            \begin{subequations}        \label{SUBEQSQooLIGNooNAzpmi}
                \begin{align}
                    | \sum_{k=0}^N(-1)^kL_k(x) |&\leq \sum_{k=0}^N| L_k(x) |\\
                    &=\sum_{k=0}^N\big|   \int_{k\pi}^{(k+1)\pi}(1- e^{-tx})\frac{ | \sin(t) | }{ t }dt   \big| \label{EQooIIVVooJvHFhS}\\
                    &\leq \sum_{k=0}^N\int_{k\pi}^{(N+1)\pi}(1- e^{-tx})\frac{ | \sin(t) | }{ t }dt\\
                    &=\int_0^{(N+1)\pi}(1- e^{-tx})\frac{ | \sin(t) | }{ t }dt\\
                    &\leq \int_0^{(N+1)\pi}tx\frac{ | \sin(t) | }{ t }dt    \label{SUBEQooZPVVooTBZzdl}\\
                    &\leq \int_0^{(N+1)\pi}xdt\\
                    &=x(N+1)\pi.
                \end{align}
            \end{subequations}
            Justifications :
            \begin{itemize}
                \item Pour \eqref{EQooIIVVooJvHFhS} c'est la définition \eqref{EQooJXWMooRbbCtt}.
                \item Pour \eqref{SUBEQooZPVVooTBZzdl}, c'est le fait que \( 0\leq 1- e^{-u}\leq u\) pour tout \( u\geq 0\) ainsi que la sous-additivité de l'intégrale de la proposition \ref{PropOPSCooVpzaBt}.
            \end{itemize}

        \item[Majoration pour l'intégrale]
            
            Nous fixons \( N\in \eN\), et nous avons :
            \begin{subequations}        \label{SUBEQSooSBSJooMAkJPh}
                \begin{align}
                    \big| \lim_{b\to \infty} \int_0^b(1-e^{-tx})\frac{ \sin(t) }{ t } dt\big|&=\big| \sum_{k=0}^{\infty}(-1)^kL_k(x) \big|\\
                    &=\big| \sum_{k=0}^N(-1)^kL_k(x)+\sum_{k=N+1}^{\infty}(-1)^kL_k(x) \big|\\
                    &\leq\big| \sum_{k=0}^N(-1)^kL_k(x) \big|+\big| \sum_{k=N+1}^{\infty}(-1)^kL_k(x) \big|\\
                    &\leq\big| \sum_{k=0}^N(-1)^kL_k(x) \big|+L_{N+1}(x)    \label{SUBEQooYFWDooYKhYtd}\\
                    &\leq\big| \sum_{k=0}^N(-1)^kL_k(x) \big|+\frac{1}{ N+1 }    \label{SUBEQooDDRGooNDfxqO}\\
                \end{align}
            \end{subequations}
            Justifications :
            \begin{itemize}
                \item Pour \eqref{SUBEQooYFWDooYKhYtd} c'est le reste du critère des séries alternées, théorème \ref{THOooOHANooHYfkII}\ref{ITEMooWEPWooXhLMYL}.
                \item Pour \eqref{SUBEQooDDRGooNDfxqO} c'est la majoration \eqref{EQooCGAQooDSvbln} déjà faite.
            \end{itemize}

        \item[Les deux ensemble]
            Pour chaque \( N\) et pour chaque \( x>0\) nous avons, en mettant \eqref{SUBEQSQooLIGNooNAzpmi} au bout de \eqref{SUBEQSooSBSJooMAkJPh} :
            \begin{equation}
                \big| \int_0^{\infty}(1- e^{-tx})\frac{ \sin(t) }{ t }dt \big|\leq x(N+1)\pi+\frac{1}{ N+1 }.
            \end{equation}
            En prenant la limite \( x\to 0\) nous trouvons
            \begin{equation}
                \lim_{x\to 0} \int_0^{\infty}(1- e^{-tx})\frac{ \sin(t) }{ t }dt\leq \frac{1}{ N+1 }
            \end{equation}
            pour tout \( N\). Donc cette limite est nulle :
            \begin{equation}
                \lim_{x\to 0} \int_0^{\infty}(1- e^{-tx})\frac{ \sin(t) }{ t }dt=0.
            \end{equation}
    \end{subproof}
\end{proof}

Maintenant que nous avons fait plein de lemmes, nous pouvons énoncer notre résultat principal, et le démontrer facilement.

\begin{theorem}[Intégrale de Dirichlet\cite{BIBooCFXJooWrArNT}]
    Nous avons
    \begin{equation}
        \lim_{b\to \infty} \int_0^b\frac{ \sin(t) }{ t }dt=\frac{ \pi }{2}.
    \end{equation}
\end{theorem}
\index{intégrale de Dirichlet}

Nous avons écrit \( \lim_{b\to \infty} \int_0^b\frac{ \sin(t) }{ t }dt\) et non \( \int_0^{\infty}\frac{ \sin(t) }{ t }dt\) parce que cette dernière intégrale n'existe pas vraiment au sens de Lebesgue, voir le lemme \ref{LEMooBEQRooHaugKj}. Dans la suite nous écrirons cependant \( \int_0^{\infty}\frac{ \sin(t) }{ t }dt\), en gardant en tête que cela n'est défini que via la limite.

\begin{proof}
    Nous nommons \( D\) la valeur que nous cherchons. Le lemme \ref{LEMooTFVZooRAmjUN} nous assure que
    \begin{equation}
        D=\lim_{b\to \infty} \int_0^b\frac{ \sin(t) }{ t }<\infty.
    \end{equation}
    Le lemme \ref{LEMooEOYHooVIMCCa} nous donne, quant à lui,
    \begin{equation}
        \lim_{b\to \infty} \int_0^b e^{-tx}\frac{ \sin(t) }{ t }dt=-\arctan(x)+\frac{ \pi }{2}.
    \end{equation}
    Vu que les deux limites existent, on peut permuter somme et limite\footnote{C'est une phrase un peu grandiloquente pour dire que \( \lim_{b\to a} f(b)-\lim_{b\to a} g(b)=\lim_{b\to a} (f(b)-g(b))  \). Ici nous avons \( a=\infty\) et les fonctions \( f\) et \( g\) sont celles définies par les intégrales.} :
    \begin{subequations}
        \begin{align}
            D+\arctan(x)-\frac{ \pi }{2}&=\lim_{b\to \infty} \big(    \int_0^b\frac{ \sin(t) }{ t } +  \int_0^b e^{-tx}\frac{ \sin(t) }{ t }dt  \big)\\
            &=\lim_{b\to \infty} \int_0^b(1- e^{-xt})\frac{ \sin(t) }{ t }dt.    \label{SUBEQooXIUNooPZnCPb}
        \end{align}
    \end{subequations}
     Pour \eqref{SUBEQooXIUNooPZnCPb}, nous avons des fonctions bornées sur un intervalle borné (\( \mathopen] 0 , b \mathclose[\)), donc il n'y a pas de mal à sommer les intégrales.
         
         Donc pour tout \( x>0\), nous avons
         \begin{equation}
             D+\arctan(x)-\frac{ \pi }{2}=\int_0^{\infty}(1- e^{-xt})\frac{ \sin(t) }{ t }dt.
         \end{equation}
         Nous passons à la limite \( x\to 0\) en utilisant le lemme \ref{LEMooNZVSooDbZCZx} et le fait que \( \arctan(0)=0\) (lemme \ref{LEMooPQNCooDkEUyw}) :
         \begin{equation}
             D-\frac{ \pi }{2}=0,
         \end{equation}
         c'est à dire que résultat annoncé.
\end{proof}

%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\section{Table de caractères du groupe diédral}
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\label{SecWMzheKf}
Cette section vient de \cite{KXjFWKA}; nous avons comme but d'établir la table des caractères des représentations complexes du groupe diédral \( D_n\).
\index{groupe!de permutation}
\index{groupe!diédral!générateurs (utilisation)}
\index{représentation!groupe diédral}
\index{caractère!groupe diédral}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Représentations de dimension un}
%---------------------------------------------------------------------------------------------------------------------------

Nous nous occupons des représentations de \( D_n\) sur \( \eC\). Les applications linéaires \( \eC\to \eC\) sont seulement les multiplications par des nombres complexes. Nous cherchons donc \( \psi\colon D_n\to \eC^*\).

Nous savons que \( D_n\) est généré\footnote{Voir proposition~\ref{PropLDIPoZ} et tout ce qui suit.} par \( s\) et \( r\). Vu que \( s^2=1\), nous avons
\begin{equation}
    \psi(s)^2=\psi(s^2)=\psi(1)=1,
\end{equation}
donc \( \psi(s)\in\{ -1,1 \}\). Nous savons aussi que \( srsr=1\), donc
\begin{equation}
    \psi(s)^2\psi(r)^2=1,
\end{equation}
ce qui donne \( \psi(r)\in\{ -1,1 \}\).

Nous avons donc quatre représentations de dimension un données par
\begin{equation*}
    \begin{array}[]{|c||c|c|}
        \hline
        &\psi(r)=1&\psi(r)=-1\\
        \hline\hline
        \psi(s)=1&\rho^{++}&\rho^{+-}\\
        \hline
        \psi(s)=-1&\rho^{-+}&\rho^{--}\\
        \hline
    \end{array}
\end{equation*}
Attention au fait que nous devons aussi avoir la relation \( \psi(r)^n=\psi(r^n)=1\). Donc \( \psi(r)\) doit être une racine \( n\)\ieme\ de l'unité. Nous allons donc devoir avoir un compte différent selon la parité de \( n\). Nous en reparlerons à la fin, au moment de faire les comptes. En ce qui concerne les caractères correspondants,
\begin{equation*}
    \begin{array}[]{|c||c|c|}
        \hline
        &r^k&sr^k\\
        \hline\hline
        \chi^{++}&1&1\\
        \hline
        \chi^{+-}&(-1)^k&(-1)^k\\
        \hline
        \chi^{-+}&1&-1\\
        \hline
        \chi^{--}&(-1)^k&(-1)^{k+1}\\
        \hline
    \end{array}
\end{equation*}
Étant donné qu'ils sont tous différents, ce sont des représentations deux à deux non équivalentes, lemme~\ref{LempUSOlo}.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Représentations de dimension deux}
%---------------------------------------------------------------------------------------------------------------------------

Nous cherchons maintenant les représentations \( \rho\colon D_n\to \End(\eC^2)\). Ici nous supposons connue la liste des éléments de \( D_n\) donnée par le corolaire~\ref{CorWYITsWW}. Soit \( \omega= e^{2i\pi/n}\) et \( h\in \eZ\); nous considérons la représentation \( \rho^{(h)}\) de \( D_n\) définie par
\begin{subequations}
    \begin{align}
        \rho^{(h)}(r^k)&=\begin{pmatrix}
            \omega^{hk}    &   0    \\
            0    &   \omega^{-hk}
        \end{pmatrix}\\
        \rho^{(h)}(st^k)&=\begin{pmatrix}
            0    &   \omega^{-hk}    \\
            \omega^{hk}    &   0
        \end{pmatrix}.
    \end{align}
\end{subequations}
Cela donne bien \( \rho^{(h)}\) sur tous les éléments de \( D_n\) par la proposition~\ref{PropLDIPoZ}. Nous pouvons restreindre le domaine de \( h\) en remarquant d'abord que \( \rho^{(h)}=\rho^{(h+n)}\), et ensuite que les représentations \( \rho^{(h)}\) et \( \rho^{(-h)}\) sont équivalentes. Un opérateur d'entrelacement est donné par \( T=\begin{pmatrix}
    0    &   1    \\
    1    &   0
\end{pmatrix}\), et il est facile de vérifier que \( T\rho^{(h)}(x)=\rho^{-h}(x)T\) avec \( x=r^k\) puis avec \( x=sr^k\).

Donc \( \rho^{(h)}\simeq\rho^{(-h)}\simeq\rho^{(n-h)}\) et nous pouvons restreindre notre étude à \( 0\leq h\leq \frac{ n }{2}\).

Nous allons séparer les cas \( n=0\), \( h=n/2\) et les autres. En effet si nous notons par commodité \( a=\omega^h\), alors un vecteur \( (x,y)\) est vecteur propre de \( \rho^{(h)}(s)\) et de \( \rho^{(h)}(r)\) si et seulement s'il vérifie les systèmes d'équations
\begin{subequations}        \label{SubEqsGXZoxLq}
    \begin{numcases}{}
        ax=\lambda x\\
        \frac{1}{ a }y=\lambda y
    \end{numcases}
\end{subequations}
et
\begin{subequations}    \label{SubEqsFYZmzhT}
    \begin{numcases}{}
        \frac{1}{ a }y=\mu x\\
        ax=\mu y
    \end{numcases}
\end{subequations}
avec \( \lambda\) et \( \mu\) des nombres non nuls. Une représentation sera réductible si et seulement si ces deux systèmes acceptent une solution non nulle commune. Il est vite vu que si \( x\neq 0\) et \( y\neq 0\), alors \( a^2=1\), ce qui signifie \( h=0\) ou \( h=n/2\). Sinon, il n'y a pas de solutions, et la représentation associée est irréductible.

\begin{enumerate}
    \item
        \( h=0\). Nous avons
        \begin{equation}
            \begin{aligned}[]
                \rho^{(0)}(r^k)&=\begin{pmatrix}
                    1    &   0    \\
                    0    &   1
                \end{pmatrix}& \rho^{(0)}(sr^k)=\begin{pmatrix}
                    0    &   1    \\
                    1    &   0
                \end{pmatrix},
            \end{aligned}
        \end{equation}
        donc le caractère de cette représentation est \( \chi^{(0)}(r^k)=2\) et \( \chi^{(0)}(sr^k)=0\). Donc nous avons
        \begin{equation}
            \chi^{(0)}=\chi^{++}+\chi^{-+}.
        \end{equation}
        Il y a maintenant (au moins) quatre façons de voir que la représentation \( \rho^{(0)}\) est réductible.
        \begin{description}

            \item[Première méthode]
                Trouver un opérateur d'entrelacement. Pour cela nous calculons les matrices :
        \begin{subequations}
            \begin{align}
                S(r)&=(\rho^{++}\oplus \rho^{-+})(r^k)=\begin{pmatrix}
                    \rho^{++}(r^k)    &   0    \\
                    0  &   \rho^{-+}(r^k)
                \end{pmatrix}=\begin{pmatrix}
                    1    &   0    \\
                    0    &   1
                \end{pmatrix}\\
                S(sr^k)&=(\rho^{++}\oplus \rho^{-+})(sr^k)=\begin{pmatrix}
                    \rho^{++}(sr^k)    &   0    \\
                    0  &   \rho^{-+}(sr^k)
                \end{pmatrix}=\begin{pmatrix}
                    1    &   0    \\
                    0    &   -1
                \end{pmatrix}\\
            \end{align}
        \end{subequations}
        Nous cherchons une matrice \( T\) telle que \( TS(r^k)=\rho^{(0)}(r^k)T\) et \( TS(sr^k)=\rho^{(0)}(sr^k)T\). Étant donné que \( S(r^k)=\mtu=\rho^{(0)}(r^k)\), la première contrainte n'en est pas une. Nous pouvons vérifier qu'avec \( T=\begin{pmatrix}
            1    &   1    \\
            1    &   -1
        \end{pmatrix}\), nous avons bien
        \begin{equation}
            T\begin{pmatrix}
                1    &   0    \\
                0    &   -1
            \end{pmatrix}=\begin{pmatrix}
                0    &   1    \\
                1    &   0
            \end{pmatrix}.
        \end{equation}
        Donc ce \( T\) entrelace \( \rho^{++}\oplus \rho^{-+}\) avec \( \rho^{(0)}\) qui sont donc deux représentations équivalentes. Donc \( \rho^{(0)}\) est réductible et ça ne nous intéresse pas de la lister.
            \item[Seconde méthode]
                Invoquer le théorème \ref{ThoWGkfADd}\ref{ItemZReOWoHi} et dire que les représentations sont équivalentes parce que les caractères sont égaux.

    \item[Troisième méthode]
        Utiliser le théorème~\ref{ThoWGkfADd}\ref{ItemZReOWoHii} et calculer \( \langle \chi^{(0)}, \chi^{(0)}\rangle \) :
        \begin{subequations}
            \begin{align}
                \langle \chi^{(0)}, \chi^{(0)}\rangle &=\frac{1}{ | D_n | }\sum_{g\in D_n}| \chi^{(0)}(g) |^2\\
                &=\frac{1}{ 2n }\big(4+0+4(n-1)\big)\\
                &=2.
            \end{align}
        \end{subequations}
        Ici le \( 4\) est pour le \( 1\), le zéro est pour les termes \( sr^k\) et \( 4(n-1)\) est pour les \( n-1\) termes \( r^k\). Vu que le résultat n'est pas \( 1\), la représentation \( \rho^{(0)}\) n'est pas irréductible.

    \item[Quatrième méthode]
        Regarder les solutions des systèmes \eqref{SubEqsGXZoxLq} et \eqref{SubEqsFYZmzhT} dont nous avons parlé plus haut.

    \end{description}

    La première méthode a l'avantage d'être simple et ne demander aucune théorie particulière à part les définitions. La seconde méthode est la plus rapide, mais demande un théorème très puissant. La troisième utilise également un théorème assez avancé, mais a l'avantage sur les deux autres méthodes de ne pas avoir besoin de savoir à priori un candidat décomposition de \( \rho^{0)}\); cette méthode est applicable même sans faire la remarque que \( \chi^{(0)}=\chi^{++}+\chi^{-+}\).

    Quoi qu'il en soit, nous ne listons pas \( \chi^{(0)}\) dans notre \href{http://fr.wikipedia.org/wiki/Aide:Unicode}{table de caractères}.

    \item
        \( h=n/2\). Vu que \( \omega^{n/2}= e^{i\pi}=-1\), nous avons
        \begin{equation}
            \begin{aligned}[]
                \rho^{(n/2)}(r^k)&=\begin{pmatrix}
                    (-1)^k    &   0    \\
                    0    &   (-1)^k
                \end{pmatrix}&
                \rho^{(n/2)}(sr^k)&=\begin{pmatrix}
                    0   &   (-1)^k    \\
                    (-1)^k    &  0
                \end{pmatrix}&
            \end{aligned},
        \end{equation}
        et donc
        \begin{subequations}
            \begin{align}
                \chi^{(n/2)}(r^k)&=2(-1)^k\\
                \chi^{(n/2)}(sr^k)&=0.
            \end{align}
        \end{subequations}
        Il est vite vu que \( \chi^{(n/2)}=\chi^{+-}+\chi^{-+}\). Ergo la représentation \( \rho^{(n/2)}\) n'est pas irréductible.

    \item
        \( 0<h<\frac{ n }{2}\). Dans ce cas nous avons \( \omega^h\neq \omega^{-h}\), et en regardant les systèmes d'équations donnés plus haut, nous voyons que \( \rho^{(h)}(s)\) et \( \rho^{(h)}(r)\) n'ont pas de vecteurs propres communs. Donc ces représentations sont irréductibles.

        Nous devons cependant encore vérifier si elles sont deux à deux non équivalentes. Supposons que pour \( h\neq h'\) nous ayons une matrice \( T\in \GL(2,\eC)\) telle que \( T\rho^{(h)}(r)T^{-1}=\rho^{(h')}(r)\). Cela impliquerait en particulier que les matrices \( \rho^{(h)}(r)\) et \( \rho^{(h')}(r)\) aient même valeurs propres. Nous aurions donc \( \{ \omega^h,\omega^{-h} \}=\{ \omega^{h'},\omega^{-h'} \}\). Mais cela est impossible avec \( 0<h<h'<\frac{ n }{2}\). Donc toutes ces représentations sont distinctes.

\end{enumerate}

Le caractère de la représentation \( \rho^{(h)}\) est \( \chi^{(h)}(r^k)=\omega^{hk}+\omega^{-hk}=2\cos\left( \frac{ 2\pi hk }{ n } \right)\).

Nous ajoutons donc la ligne suivante à notre liste :
\begin{equation*}
    \begin{array}[]{|c||c|c|}
        \hline
        &r^k&sr^k\\
        \hline\hline
        \chi^{(h)}&2\cos\left( \frac{ 2\pi hk }{ n } \right)&0\\
        \hline
    \end{array}
\end{equation*}

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Le compte pour \texorpdfstring{$ n$}{n} pair}
%---------------------------------------------------------------------------------------------------------------------------

Nous avons \( 4\) représentations de dimension \( 1\) puis \( \frac{ n }{2}-1\) représentations de dimension \( 2\). En tout nous avons
\begin{equation}
 \frac{ n }{2}+3
\end{equation}
représentations irréductibles modulo équivalence. Cela fait le compte en vertu des classes de conjugaisons listées en~\ref{SubsubsecROVmHuM}. Pour rappel, le nombre de représentations non équivalentes est égal au nombre de classes de conjugaison par le corolaire~\ref{CorbdcVNC}. Notons que c'est cela qui justifie le fait que nous ne devons pas chercher d'autres représentations. Nous sommes sûrs de les avoir toutes trouvées.

%---------------------------------------------------------------------------------------------------------------------------
\subsection{Le compte pour \texorpdfstring{$ n$}{n} impair}
%---------------------------------------------------------------------------------------------------------------------------

Nous avions fait mention plus haut du fait que si \( \psi\) est une représentation de dimension \( 1\), le nombre \( \psi(r)\) devait être une racine \( n\)\ieme\ de l'unité. Donc en dimension \( 1\) nous avons seulement les représentations \( \rho^{++}\) et \( \rho^{-+}\). Pour celles de dimension \( 2\), nous en avons \( \frac{ n-1 }{2}\). En tout nous avons donc
\begin{equation}
    \frac{ n+3 }{2}
\end{equation}
représentations irréductibles modulo équivalence. Cela fait le compte en vertu des classes de conjugaisons listées en~\ref{GJIzDEP}.
